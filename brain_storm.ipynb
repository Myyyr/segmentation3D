{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# GPU\n",
    "gpu = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "def convert_bytes(size):\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size < 1024.0:\n",
    "            return \"%3.2f %s\" % (size, x)\n",
    "        size /= 1024.0\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 512, 512] 512.00 MB\n",
      "[32, 32, 32, 4096] 512.00 MB\n",
      "[32768, 4096] 512.00 MB\n",
      "[32768, 32768] 4.00 GB\n"
     ]
    }
   ],
   "source": [
    "v_size = [512,512,512]\n",
    "print(v_size, convert_bytes(np.prod(v_size)*4))\n",
    "ptch_size = [16,16,16]\n",
    "\n",
    "emb_size_reshape = [int(i/j) for i,j in zip(v_size, ptch_size)] + [np.prod(ptch_size)]\n",
    "print(emb_size_reshape, convert_bytes(np.prod(emb_size_reshape)*4))\n",
    "emb_size_flat = [np.prod(emb_size_reshape[:3]), emb_size_reshape[3]]\n",
    "print(emb_size_flat, convert_bytes(np.prod(emb_size_flat)*4))\n",
    "att_size = [emb_size_flat[0]]*2\n",
    "print(att_size, convert_bytes(np.prod(att_size)*4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 4.38 GB 4.26 GB\n",
      "Input 4.76 GB 4.76 GB\n",
      "Input reshape 4.76 GB 4.76 GB\n",
      "Input flat 4.76 GB 4.76 GB\n",
      "torch.Size([1, 512, 262144])\n",
      "Model 13.88 GB 13.88 GB\n",
      "Forward 14.10 GB 9.83 GB\n",
      "y.shape torch.Size([1, 512, 8192])\n"
     ]
    }
   ],
   "source": [
    "print('Init',convert_bytes(torch.cuda.max_memory_allocated()), convert_bytes(torch.cuda.memory_allocated()))\n",
    "bs = 1\n",
    "v_size = [bs,512,512,512]\n",
    "ptch_size = [64]*3\n",
    "emb_size_reshape = [bs] + [int(i/j) for i,j in zip(v_size[1:], ptch_size)] + [np.prod(ptch_size)]\n",
    "emb_size_flat = [bs] + [np.prod(emb_size_reshape[1:4]), emb_size_reshape[-1]]\n",
    "d_model = 4096*2#emb_size_reshape[-1]\n",
    "\n",
    "x = torch.from_numpy(np.random.rand(*v_size).astype(float)).float().cuda()\n",
    "print('Input',convert_bytes(torch.cuda.max_memory_allocated()), convert_bytes(torch.cuda.memory_allocated()))\n",
    "x = torch.reshape(x, emb_size_reshape)\n",
    "print('Input reshape',convert_bytes(torch.cuda.max_memory_allocated()), convert_bytes(torch.cuda.memory_allocated()))\n",
    "x = torch.reshape(x, emb_size_flat)\n",
    "print('Input flat',convert_bytes(torch.cuda.max_memory_allocated()), convert_bytes(torch.cuda.memory_allocated()))\n",
    "print(x.shape)\n",
    "\n",
    "lin = nn.Linear(emb_size_flat[-1], d_model).float().cuda()\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8)\n",
    "tr = nn.TransformerEncoder(encoder_layer, num_layers=1).float().cuda()\n",
    "print('Model',convert_bytes(torch.cuda.max_memory_allocated()), convert_bytes(torch.cuda.memory_allocated()))\n",
    "\n",
    "x = lin(x)\n",
    "y = tr(x)\n",
    "print('Forward',convert_bytes(torch.cuda.max_memory_allocated()), convert_bytes(torch.cuda.memory_allocated()))\n",
    "\n",
    "print('y.shape', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = [8, 16, 32, 32, 32, 32]\n",
    "d_ = [1024, 1024, 1024, 2048, 4096, 8192]\n",
    "mem = [18.56, 2.8, 0.96, 1.34, 2.23, 4.38] #GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32.00 GB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes(4*64*512**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 9, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0\u001b[K\n",
      "Dépaquetage des objets: 100% (5/5), 420 octets | 420.00 Kio/s, fait.\n",
      "Depuis github.com:Myyyr/segmentation3D\n",
      "   0b46865..b6cd49b  main       -> origin/main\n",
      "Mise à jour 0b46865..b6cd49b\n",
      "Fast-forward\n",
      " models/mymod/UNETR.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# GPU\n",
    "gpu = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "def convert_bytes(size):\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size < 1024.0:\n",
    "            return \"%3.2f %s\" % (size, x)\n",
    "        size /= 1024.0\n",
    "    return size\n",
    "\n",
    "import models.mymod.UNETR as unetr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNETR\n"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "input_shape = [512,512,256]\n",
    "v_size = [bs,1]+ input_shape\n",
    "filters = [4, 16, 64, 256]\n",
    "skip_idx = [3,6,9,12]\n",
    "patch_size=(16,16,16)\n",
    "n_layers=12\n",
    "#ptch_size = [64]*3\n",
    "#d_model = 4096*2#emb_size_reshape[-1]\n",
    "\n",
    "x = torch.from_numpy(np.random.rand(*v_size).astype(float)).float().cuda()\n",
    "\n",
    "mod = unetr.UNETR(input_shape=input_shape,filters=filters,patch_size=patch_size, n_layers=n_layers, skip_idx=skip_idx).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512, 512, 256])\n",
      "Forward 18.89 GB 17.34 GB\n"
     ]
    }
   ],
   "source": [
    "y = mod(x)\n",
    "print(y.shape)\n",
    "print('Forward',convert_bytes(torch.cuda.max_memory_allocated()), convert_bytes(torch.cuda.memory_allocated()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
